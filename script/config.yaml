# 配置多个模型
# 每个模型可以配置多个worker,
# 非 hf 工作模式 每个worker可以配置多个device_id

serve_args:
  host: 0.0.0.0
  port: 8081
  # http 工作进程
  workers: 4

  api_keys: []
#  api_keys:
#    - 112233



chatglm3-6b:
  alias: null
  # 当仅有一个lora, 是否自动合并成一个模型，此方法将无法调用基础模型，只能使用合并后的权重
  auto_merge_lora_single: true
  # 是否自动量化模型
  auto_quantize: false
  # 是否启用
  enable: false
  # embdding max batch size
  max_batch_size: 1
  model_config:
    do_lower_case: null
    # 是否启用lora , 多个 lora , adapter_name: lora weight dir
    # "default": "/data/nlp/pre_models/torch/lora_path_dir"
    # "your_adapter_name": "/data/nlp/pre_models/torch/your_adapter_dir"
    lora: {}
    model_name_or_path: /data/nlp/pre_models/torch/chatglm3/chatglm3-6b
    model_type: chatglm
    use_fast_tokenizer: false
  # 扩展位置长度 1 即为 1 * 2048 不扩充 , 2 即为 2 * 2 * 2048 以此类推
  ntk_scale: 1

  # one of deepspeed,accelerate,hf    单卡建议使用 hf
  work_mode: deepspeed


  workers:
  # 第一个worker
  - device_id:
    # 使用第一块和第二块显卡
    - 0
    - 1
#  # 第二个worker
#  - device_id:
#    # 使用第三块和第四块显卡
#    - 2
#    - 3





Qwen-7B-Chat:
  # 当仅有一个lora, 是否自动合并成一个模型，此方法将无法调用基础模型，只能使用合并后的权重
  auto_merge_lora_single: true
  # 是否自动量化模型
  auto_quantize: false
  # 是否启用
  enable: false
  # embdding max batch size
  max_batch_size: 1
  model_config:
    do_lower_case: null
    # 是否启用lora , 多个 lora , adapter_name: lora weight dir
    # "default": "/data/nlp/pre_models/torch/lora_path_dir"
    # "your_adapter_name": "/data/nlp/pre_models/torch/your_adapter_dir"
    lora: {}
    model_name_or_path: /data/nlp/pre_models/torch/qwen/Qwen-7B-Chat
    model_type: qwen
    use_fast_tokenizer: false

  # one of deepspeed,accelerate,hf    单卡建议使用 hf
  work_mode: deepspeed


  workers:
  # 第一个worker
  - device_id:
    # 使用第一块和第二块显卡
    - 0
    - 1
#  # 第二个worker
#  - device_id:
#    # 使用第三块和第四块显卡
#    - 2
#    - 3



# 向量模型
bge-base-zh-v1.5:

  # 别名 例如 ["gpt-3.5-turbo","gpt-4"], # 别名 str or list[str]
  "alias": null

  # 当仅有一个lora, 是否自动合并成一个模型，此方法将无法调用基础模型，只能使用合并后的权重
  auto_merge_lora_single: true
  # 是否自动量化模型
  auto_quantize: false
  # 是否启用
  enable: false
  # embdding max batch size
  max_batch_size: 1

  # 扩展位置长度 1 即为 1 * 2048 不扩充 , 2 即为 2 * 2 * 2048 以此类推
# 扩展位置长度 1 即为 1 * 2048 不扩充 , 4 即为 4 * 2048 以此类推,
  model_config:
    do_lower_case: null
  # 是否启用

    # 多个 lora , adapter_name: lora weight dir
    # "default": "/data/nlp/pre_models/torch/lora_path_dir"
    # "your_adapter_name": "/data/nlp/pre_models/torch/your_adapter_dir"
    lora: {}
    model_name_or_path: /home/share/nlp/pre_models/torch/bge/bge-base-zh-v1.5
    model_type: bert
    use_fast_tokenizer: false

  # one of deepspeed,accelerate,hf    单卡建议使用 hf
  work_mode: deepspeed



  workers:
  # 第一个worker
  - device_id:
    # 使用第一块和第二块显卡
    - 0
#  # 第二个worker
#  - device_id:
#    # 使用第三块和第四块显卡
#    - 2
#    - 3

